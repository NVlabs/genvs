<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="media/graphics/favicon.ico" rel="shortcut icon" />
    <title> GeNVS </title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="box_swipe.css">
    <script src="box_swipe.js"></script>
    <link href="https://fonts.googleapis.com/css?family=Montserrat|Segoe+UI" rel="stylesheet" />
</head>

<body>
    <!-- SECTION: HEADER -->
    <div class="n-header">
    </div>
    <div class="n-title">
        <h1> GeNVS: Generative Novel View Synthesis with <br>3D-Aware Diffusion Models </h1>
    </div>
    <!-- SECTION: AUTHORS -->
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li> <a href="https://ericryanchan.github.io" target="_blank">Eric R. Chan</a> <sup> * 1, 2 </sup>
                </li>
                <li> <a href="https://luminohope.org" target="_blank">Koki Nagano</a> <sup> * 2 </sup>
                </li>
                <li> <a href="https://matthew-a-chan.github.io" target="_blank">Matthew A. Chan</a> <sup> * 2 </sup>
                </li>
                <li> <a href="https://alexanderbergman7.github.io" target="_blank">Alexander W. Bergman</a> <sup> * 1 </sup>
                </li>
                <li> <a href="https://jjparkcv.github.io" target="_blank">Jeong Joon Park</a> <sup> * 1 </sup>
                </li>
                <li> <a href="https://axlevy.com" target="_blank">Axel Levy</a>
                    <sup> 1 </sup>
                </li>
                <li> <a href="https://research.nvidia.com/person/miika-aittala" target="_blank">Miika Aittala</a> <sup> 2 </sup>
                </li>
                <li> <a href="https://research.nvidia.com/person/shalini-gupta" target="_blank">Shalini De Mello</a> <sup> 2
                    </sup>
                </li>
                <li> <a href="https://research.nvidia.com/person/tero-karras" target="_blank">Tero Karras</a> <sup> 2 </sup>
                </li>
                <li> <a href="https://stanford.edu/~gordonwz" target="_blank">Gordon Wetzstein</a> <sup> 1 </sup>
                </li>
            </ul>
            <div class="authors-affiliations-gap"></div>
            <ul class="authors affiliations">
                <li>
                    <sup> 1 </sup> Stanford University
                </li>
                <li>
                    <sup> 2 </sup> NVIDIA
                </li>
            </ul>
            <ul class="authors affiliations">
                <li>
                    <sup> * </sup> Equal contribution.
                </li>
            </ul>
        </div>
    </div>
    <!-- SECTION: MAIN BODY -->
    <div class="n-article">
        <!-- teaser -->
        <div class="l-article video">
            <video controls="" loop="" width="100%" autoplay muted>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="media/teaser.mp4#t=1"
                    type="video/mp4" />
            </video>
        </div>
        <!-- abstract -->
        <h2 id="abstract"> Abstract </h2>
        <p> We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method's ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.
 </p>
        <!-- paper links -->
        <h2 id="links"> Links </h2>
        <div class="grid download-section">
            <div class="download-thumb">
                <a href="media/genvs.pdf" target="_blank">
                    <img class="dropshadow" src="media/paper_thumbnail.jpg" />
                </a>
            </div>
            <div class="download-links">
                <ul>
                    <li>
                        <a href="media/genvs.pdf" target="_blank"> Paper PDF </a>
                    </li>
                    <!-- <li> -->
                        <!-- <a href="XXXXXXXX" target="_blank"> arXiv </a> -->
                    <!-- </li> -->
                    <li>
                        <a href="https://github.com/NVlabs/genvs" target="_blank"> Code on GitHub (Coming Soon) </a>
                    </li>
                </ul>
            </div>
        </div>

        <h2>Overview</h2>
        </p>
        <div class="l-article video">
            <img src="media/network-diagram.jpg" width="100%" />
            <div class="videocaption">
                <div>
                </div>
            </div>
        </div>
        <p>
        At a high level, our model operates as a conditional diffusion model for images, much like the models
        that have been successful in image inpainting, superresolution, and other conditional image generation tasks.
        Conditioned on an input view, we generate novel views by progressively denoising a sample of Gaussian noise.
        However, in this work, we embed 3D priors into the architecture in the form of a 3D feature field,
        which enhances the model's ability to synthesize views on complex scenes.
        <p>
        We lift and aggregate features from input image(s) into a 3D feature field. Given a query viewpoint, we volume-render
        a feature image to condition a U-Net image denoiser. The entire model, including feature encoder, volume renderer, and
        U-Net components, is trained end-to-end as an image-conditional diffusion model.
        At inference, we generate consistent sequences in an auto-regressive fashion.</p>
        <p>

        <h2 id="videos"> Videos </h2>
        <p> The following videos demonstrate novel view synthesis with our method, which produces high-quality,
            multi-view-consistent renderings on varied datasets. </p>

        <div class="l-article video">
            <video controls="" loop="" width="100%" autoplay muted>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="media/co3d.mp4#t=1"
                    type="video/mp4" />
            </video>
            <div class="videocaption">
                <div>
                    We demonstrate our method achieves compelling single-image novel view synthesis
                    results on challenging, unmasked scenes from the Common Objects in 3D Dataset. To our knowledge, ours is the first
                    work to attempt single-image novel view synthesis on this benchmark without object masks. 
                </div>
            </div>
        </div>
        <p>
        <div class="l-article video">
            <video controls="" loop="" width="100%" autoplay muted>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="media/mp3d.mp4#t=1"
                    type="video/mp4" />
            </video>
            <div class="videocaption">
                <div>
                    Flexibility is a strength of our approach. Beyond object-centric scenes,
                    our method can also operate on large, inside-out scenes, such as these room-scale scenes from the
                    Matterport3D dataset. By autoregressively generating frames, we can explore far from the input pose. 
                </div>
            </div>
        </div>
        <p>
        <div class="l-article video">
            <video controls="" loop="" width="100%" autoplay muted>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="media/shapenet.mp4#t=1" type="video/mp4" />
            </video>
            <div class="videocaption">
                <div>
                    Our method is competitive with state-of-the-art baselines for single-image
                    novel view synthesis on objects from the ShapeNet dataset.
                </div>
            </div>
        </div>
        <p>
        <h2 id="citation"> Citation </h2>
        <pre><code>@inproceedings{chan2023genvs,
            author = {Eric R. Chan and Koki Nagano and Matthew A. Chan and Alexander W. Bergman and Jeong Joon Park and Axel Levy and Miika Aittala and Shalini De Mello and Tero Karras and Gordon Wetzstein},
            title = {{GeNVS}: Generative Novel View Synthesis with {3D}-Aware Diffusion Models},
            booktitle = {arXiv},
            year = {2023}
          }</code></pre>
        <h2 id="license"> License </h2>
        <p> Images, text and video files on this site are made freely available for non-commercial use under the <a
                href="https://nvlabs-fi-cdn.nvidia.com/_web/stylegan3/LICENSE.txt"> Creative Commons CC BY-NC 4.0
                license </a> . Feel free to use any of the material in your own work, as long as you give us appropriate
            credit by mentioning the title and author list of our paper. </p>
        <h2 id="acknowledgments"> Acknowledgments </h2>
        <p> We thank David Luebke, Samuli Laine, Tsung-Yi Lin, and Jaakko Lehtinen for feedback on drafts and early discussions.
            We thank Jonáš Kulhánek and Xuanchi Ren for thoughtful communications and for providing results and data for comparisons.
            We thank Trevor Chan for help with figures. Koki Nagano and Eric Chan were partially supported by DARPA’s Semantic Forensics
            (SemaFor) contract (HR0011-20-3-0005). JJ park was supported by ARL grant W911NF-21-2-0104.
            This project was in part supported by Samsung, the Stanford Institute for Human-Centered AI (HAI), and
            a PECASE from the ARO. The views and conclusions contained in this document are those of the authors and
            should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.
            Distribution Statement ``A'' (Approved for Public Release, Distribution Unlimited).
            We base this website off of the <a
                href="https://nvlabs.github.io/stylegan3/" target="_blank">StyleGAN3</a> website template.</p>
    </div>
    <div class="n-footer">
    </div>
</body>

</html>
